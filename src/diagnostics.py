import arviz
import scipy
import numpy as np
from typing import Iterable, Callable, List
from tabulate import tabulate


def chain_convergence_diagnostics(
    chains: np.array, samples: Iterable, param_names: Iterable[str]
):
    """! Computes and displays diagnostic statistics for the Markov chain and
    the extract samples. This includes the Gelman-Rubin statistic, as well as
    measuring the effective fraction of samples.

    @param chains       The set of chains generated by a single algorithm.
    The Gelman-Rubin statistic requires at least 2 chains.
    @param samples      The accepted samples from the Markov chain.
    @param param_names  The names of all parameters."""
    # Create Arziv dataset
    dataset = {param: chains[:, :, i] for i, param in enumerate(param_names)}

    # Simple summary
    print(
        "Effective number of samples is {frac: .2f}% of the total samples".format(
            frac=len(samples) / chains.shape[0] / chains.shape[1] * 100
        )
    )

    # Compute Gelman-Rubin Diagnostic (R hat)
    r_hats = arviz.diagnostics.rhat(dataset)
    for param in param_names:
        print(
            f'Measured Gelman-Rubin statistic for parameter "{param}": {r_hats[param]}'
        )


def symmetric_kl_divergence(x: Iterable, y: Iterable):
    r"""r! Computes the symmetric KL Divergence for two samples \f$x\f$ and \f$y\f$.
    The symmetric KL Divergence is given by:
    \f$KL(P,Q) = \frac{1}{2}(KL(P||Q) + KL(Q||P)\f$, where:
    \f$KL(P||Q) = \int P(x)\log{\frac{P(x)}{Q(x)}dx}\f

    @param x    The first sample.
    @param y    The second sample.

    @returns    The symmetric KL Divergence and the associated error from the Monte Carlo integration
    """
    # Approximate distributions using Kernel Density Estimation
    dist1_estimate = scipy.stats.gaussian_kde(x.T)
    dist2_estimate = scipy.stats.gaussian_kde(y.T)

    # Monte carlo integration for KL divergence
    kl_pq = np.mean(
        np.log(dist1_estimate(x.T) + 1e-10) - np.log(dist2_estimate(x.T) + 1e-10)
    )

    # Estimate error based on MC integration
    kl_pq_err = np.std(
        np.log(dist1_estimate(x.T) + 1e-10) - np.log(dist2_estimate(x.T) + 1e-10)
    ) / np.sqrt(len(x))

    # Monte carlo integration for KL divergence
    kl_qp = np.mean(
        np.log(dist2_estimate(y.T) + 1e-10) - np.log(dist1_estimate(y.T) + 1e-10)
    )

    # Estimate error based on MC integration
    kl_qp_err = np.std(
        np.log(dist2_estimate(y.T) + 1e-10) - np.log(dist1_estimate(y.T) + 1e-10)
    ) / np.sqrt(len(y))

    return (
        kl_pq,
        kl_pq_err,
        kl_qp,
        kl_qp_err,
        (kl_pq + kl_qp) / 2,
        np.sqrt(kl_pq_err**2 + kl_qp_err**2) / 2,
    )


def kl_divergence(x: Iterable[float], pdf: Callable):
    r"""r!Computes the  KL Divergence between a sample and a true distribution.
    The KL Divergence is given by:
    \f$KL(P||Q) = \int P(x)\log{\frac{P(x)}{Q(x)}dx}\f

    @param x    The extracted sample.
    @param pdf  The target probability distribution.

    @returns    The KL Divergence and the associated error from the Monte Carlo integration
    """
    # Approximate distributions using Kernel Density Estimation
    dist1_estimate = scipy.stats.gaussian_kde(x.T)

    # Monte carlo integration for KL divergence
    kl_pq = np.mean(np.log(dist1_estimate(x.T) + 1e-10) - np.log(pdf(x) + 1e-10))

    # Estimate error based on MC integration
    kl_pq_err = np.std(
        np.log(dist1_estimate(x.T) + 1e-10) - np.log(pdf(x) + 1e-10)
    ) / np.sqrt(len(x))

    return kl_pq, kl_pq_err


def distribution_summaries(
    samples: Iterable, algo_names: List[str], true_pdf: Callable
):
    """! Displays the distribution distances using the KL divergence metric.

    @param samples      A set of data samples from different sampling algorithms.
    @param algo_names   The names of the sampling algorithms.
    @param true_pdf     The function representing the target PDF.
    """
    kl_div = np.zeros((len(algo_names), len(algo_names) + 2), dtype=object)
    sym_kl_div = np.zeros((len(algo_names), len(algo_names) + 1), dtype=object)

    # Populate row indeces
    for i in range(len(algo_names)):
        kl_div[i][0] = algo_names[i]
        sym_kl_div[i][0] = algo_names[i]

    for i in range(len(algo_names)):
        # Measure KL divergence with true distributions
        true_kl_div = kl_divergence(samples[i], true_pdf)
        kl_div[i, -1] = str(true_kl_div[0]) + "+-" + str(true_kl_div[1])

        for j in range(i + 1, len(algo_names)):
            kl_results = symmetric_kl_divergence(samples[i], samples[j])

            # Register assymetric computations
            kl_div[i, j + 1] = str(kl_results[0]) + "+-" + str(kl_results[1])
            kl_div[j, i + 1] = str(kl_results[2]) + "+-" + str(kl_results[3])

            # Register symmetric measurement
            sym_kl_div[i, j + 1] = str(kl_results[4]) + "+-" + str(kl_results[5])
            sym_kl_div[j, i + 1] = str(kl_results[4]) + "+-" + str(kl_results[5])

    headers = [""] + algo_names + ["True"]

    # Print Raw KL Divergence Measurements
    table = tabulate(kl_div, headers=headers)
    print("Kullback-Liebler Divergence metrics for samples:")
    print(table)

    # Print Symmetric KL Divergence Measurements
    table = tabulate(sym_kl_div, headers=headers)
    print("Symmetric Kullback-Liebler Divergence metrics for samples:")
    print(table)
