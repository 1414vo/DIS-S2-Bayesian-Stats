import arviz
import scipy
import numpy as np
from typing import Iterable, Callable


def chain_convergence_diagnostics(
    chains: np.array, samples: Iterable, param_names: Iterable[str]
):
    """! Computes and displays diagnostic statistics for the Markov chain and
    the extract samples. This includes the Gelman-Rubin statistic, as well as
    measuring the effective fraction of samples.

    @param chains       The set of chains generated by a single algorithm.
    The Gelman-Rubin statistic requires at least 2 chains.
    @param samples      The accepted samples from the Markov chain.
    @param param_names  The names of all parameters."""
    # Create Arziv dataset
    dataset = {param: chains[:, :, i] for i, param in enumerate(param_names)}

    # Simple summary
    print(
        "Effective number of samples is {frac: .2f}% of the total samples".format(
            frac=len(samples) / chains.shape[0] / chains.shape[1] * 100
        )
    )

    # Compute Gelman-Rubin Diagnostic (R hat)
    r_hats = arviz.diagnostics.rhat(dataset)
    for param in param_names:
        print(
            f'Measured Gelman-Rubin statistic for parameter "{param}": {r_hats[param]}'
        )


def symmetric_kl_divergence(x: Iterable, y: Iterable):
    r"""r! Computes the symmetric KL Divergence for two samples \f$x\f$ and \f$y\f$.
    The symmetric KL Divergence is given by:
    \f$KL(P,Q) = \frac{1}{2}(KL(P||Q) + KL(Q||P)\f$, where:
    \f$KL(P||Q) = \int P(x)\log{\frac{P(x)}{Q(x)}dx}\f

    @param x    The first sample.
    @param y    The second sample.

    @returns    The symmetric KL Divergence and the associated error from the Monte Carlo integration
    """
    # Approximate distributions using Kernel Density Estimation
    dist1_estimate = scipy.stats.gaussian_kde(x.T)
    dist2_estimate = scipy.stats.gaussian_kde(y.T)

    # Monte carlo integration for KL divergence
    kl_pq = np.mean(
        np.log(dist1_estimate(x.T) + 1e-10) - np.log(dist2_estimate(x.T) + 1e-10)
    )

    # Estimate error based on MC integration
    kl_pq_err = np.std(
        np.log(dist1_estimate(x.T) + 1e-10) - np.log(dist2_estimate(x.T) + 1e-10)
    ) / np.sqrt(len(x))

    # Monte carlo integration for KL divergence
    kl_qp = np.mean(
        np.log(dist2_estimate(y.T) + 1e-10) - np.log(dist1_estimate(y.T) + 1e-10)
    )

    # Estimate error based on MC integration
    kl_qp_err = np.std(
        np.log(dist2_estimate(y.T) + 1e-10) - np.log(dist1_estimate(y.T) + 1e-10)
    ) / np.sqrt(len(y))

    return (kl_pq + kl_qp) / 2, np.sqrt(kl_pq_err**2 + kl_qp_err**2) / 2


def kl_divergence(x: Iterable[float], pdf: Callable):
    r"""r!Computes the  KL Divergence between a sample and a true distribution.
    The KL Divergence is given by:
    \f$KL(P||Q) = \int P(x)\log{\frac{P(x)}{Q(x)}dx}\f

    @param x    The extracted sample.
    @param pdf  The target probability distribution.

    @returns    The KL Divergence and the associated error from the Monte Carlo integration
    """
    # Approximate distributions using Kernel Density Estimation
    dist1_estimate = scipy.stats.gaussian_kde(x.T)

    # Monte carlo integration for KL divergence
    kl_pq = np.mean(np.log(dist1_estimate(x.T) + 1e-10) - np.log(pdf(x) + 1e-10))

    # Estimate error based on MC integration
    kl_pq_err = np.std(
        np.log(dist1_estimate(x) + 1e-10) - np.log(pdf(x) + 1e-10)
    ) / np.sqrt(len(x))

    return kl_pq, kl_pq_err
